{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c259e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U scikit-learn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9f394b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 50 alphabets.\n",
      "The dataset contains 1623 characters.\n",
      "The dataset contains 32460 images.\n"
     ]
    }
   ],
   "source": [
    "PATH = \"/Users/preethi/MS DS/DeepLearning/Final Project/omniglot_combined\"\n",
    "\n",
    "# Get all alphabet folders\n",
    "alphabet_folders = [os.path.join(PATH, folder)\n",
    "                    for folder in os.listdir(PATH)\n",
    "                        if os.path.isdir(os.path.join(PATH, folder))]\n",
    "\n",
    "print(f\"The dataset contains {len(alphabet_folders)} alphabets.\")\n",
    "\n",
    "# Get all character folders\n",
    "character_folders = [os.path.join(alphabet_path, character)\n",
    "                     for alphabet_path in alphabet_folders\n",
    "                         for character in os.listdir(alphabet_path)\n",
    "                             if os.path.isdir(os.path.join(alphabet_path, character))]\n",
    "\n",
    "# Sort alphabetically \n",
    "character_folders = sorted(character_folders)\n",
    "\n",
    "print(f\"The dataset contains {len(character_folders)} characters.\")\n",
    "\n",
    "# Get all images\n",
    "all_files = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(PATH):\n",
    "    all_files += [os.path.join(dirpath, file) for file in filenames if not file.startswith('.')]\n",
    "print(f\"The dataset contains {len(all_files)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e7e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_propocess_image(img_path, image_dim=None):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns a grayscale, resized, and flattened image as array\n",
    "    \n",
    "    Args:\n",
    "    * img_path: Full path to file\n",
    "    * image_dim: Resized image shape (width, height)\n",
    "    \n",
    "    Returns:\n",
    "    a grayscale, resized, and flattened image\n",
    "    \"\"\"\n",
    "    # Read image as grayscale\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Resize image\n",
    "    if image_dim is not None:\n",
    "        img = cv2.resize(img, image_dim, interpolation = cv2.INTER_AREA) \n",
    "    \n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = 1.0 - img\n",
    "    img = img.reshape(-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0980a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_files(path, train_path, test_path, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Takes a path to a folder and splits the files into train and test folders in 80:20 ratio.\n",
    "    \n",
    "    Args:\n",
    "    * path: Full path to folder\n",
    "    * train_path: Full path to train folder\n",
    "    * test_path: Full path to test folder\n",
    "    * train_ratio: Ratio of train to test files\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get all alphabet folders\n",
    "    alphabet_folders = [os.path.join(path, folder)\n",
    "                        for folder in os.listdir(path)\n",
    "                            if os.path.isdir(os.path.join(path, folder))]\n",
    "\n",
    "    # Get all character folders\n",
    "    character_folders = [os.path.join(alphabet_path, character)\n",
    "                         for alphabet_path in alphabet_folders\n",
    "                             for character in os.listdir(alphabet_path)\n",
    "                                 if os.path.isdir(os.path.join(alphabet_path, character))]\n",
    "\n",
    "    # Sort alphabetically \n",
    "    character_folders = sorted(character_folders)\n",
    "    \n",
    "    # Create train and test folders\n",
    "    if not os.path.exists(train_path):\n",
    "        os.makedirs(train_path)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "    \n",
    "    # Loop through each character folder\n",
    "    for character_folder in character_folders:\n",
    "        # Get alphabet name and character number\n",
    "        alphabet_name = character_folder.split('/')[-2]\n",
    "        character_number = character_folder.split('/')[-1]\n",
    "        \n",
    "        # Create alphabet folder in train and test folders\n",
    "        train_alphabet_folder = os.path.join(train_path, alphabet_name)\n",
    "        test_alphabet_folder = os.path.join(test_path, alphabet_name)\n",
    "        if not os.path.exists(train_alphabet_folder):\n",
    "            os.makedirs(train_alphabet_folder)\n",
    "        if not os.path.exists(test_alphabet_folder):\n",
    "            os.makedirs(test_alphabet_folder)\n",
    "        \n",
    "        # Create character folder in train and test folders\n",
    "        train_character_folder = os.path.join(train_alphabet_folder, character_number)\n",
    "        test_character_folder = os.path.join(test_alphabet_folder, character_number)\n",
    "        if not os.path.exists(train_character_folder):\n",
    "            os.makedirs(train_character_folder)\n",
    "        if not os.path.exists(test_character_folder):\n",
    "            os.makedirs(test_character_folder)\n",
    "        \n",
    "        # Get all image files\n",
    "        files = os.listdir(character_folder)\n",
    "        \n",
    "        # Randomly shuffle files\n",
    "        random.seed(42)\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        # Split files into train and test\n",
    "        train_files = files[:int(len(files)*train_ratio)]\n",
    "        test_files = files[int(len(files)*train_ratio):]\n",
    "        \n",
    "        # Copy each file into train and test folders\n",
    "        for file in train_files:\n",
    "            shutil.copy(os.path.join(character_folder, file), train_character_folder)\n",
    "        for file in test_files:\n",
    "            shutil.copy(os.path.join(character_folder, file), test_character_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e081fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define paths to train and test folders\n",
    "train_folder = '/Users/preethi/MS DS/DeepLearning/Final Project/train_folder1'\n",
    "test_folder = '/Users/preethi/MS DS/DeepLearning/Final Project/test_folder1'\n",
    "\n",
    "# Define image size and batch size\n",
    "img_height, img_width = 28, 28\n",
    "batch_size = 32\n",
    "\n",
    "# Use ImageDataGenerator to preprocess images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data generators for train and test sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_folder,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=50)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb732fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36d7ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe:\n",
      "                                            filepath  language    character   \n",
      "0  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42  \\\n",
      "1  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "2  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "3  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "4  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "\n",
      "                  label  \n",
      "0  Gujarati_character42  \n",
      "1  Gujarati_character42  \n",
      "2  Gujarati_character42  \n",
      "3  Gujarati_character42  \n",
      "4  Gujarati_character42  \n",
      "\n",
      "Test dataframe:\n",
      "                                            filepath  language    character   \n",
      "0  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42  \\\n",
      "1  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "2  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "3  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character42   \n",
      "4  /Users/preethi/MS DS/DeepLearning/Final Projec...  Gujarati  character45   \n",
      "\n",
      "                  label  \n",
      "0  Gujarati_character42  \n",
      "1  Gujarati_character42  \n",
      "2  Gujarati_character42  \n",
      "3  Gujarati_character42  \n",
      "4  Gujarati_character45  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_folder = '/Users/preethi/MS DS/DeepLearning/Final Project/train_folder'\n",
    "test_folder = '/Users/preethi/MS DS/DeepLearning/Final Project/test_folder'\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# Loop over the alphabet folders\n",
    "for alphabet_folder in os.listdir(train_folder):\n",
    "    if not alphabet_folder.startswith('.'):\n",
    "        # Loop over the character folders\n",
    "        for character_folder in os.listdir(os.path.join(train_folder, alphabet_folder)):\n",
    "            if not character_folder.startswith('.'):\n",
    "                # Loop over the images\n",
    "                for image_file in os.listdir(os.path.join(train_folder, alphabet_folder, character_folder)):\n",
    "                    if not image_file.startswith('.'):\n",
    "                        # Append the file path and label to the train data list\n",
    "                        train_data.append([os.path.join(train_folder, alphabet_folder, character_folder, image_file), alphabet_folder, character_folder])\n",
    "\n",
    "# Loop over the alphabet folders\n",
    "for alphabet_folder in os.listdir(test_folder):\n",
    "    if not alphabet_folder.startswith('.'):\n",
    "        # Loop over the character folders\n",
    "        for character_folder in os.listdir(os.path.join(test_folder, alphabet_folder)):\n",
    "            if not character_folder.startswith('.'):\n",
    "                # Loop over the images\n",
    "                for image_file in os.listdir(os.path.join(test_folder, alphabet_folder, character_folder)):\n",
    "                    if not image_file.startswith('.'):\n",
    "                        # Append the file path and label to the test data list\n",
    "                        test_data.append([os.path.join(test_folder, alphabet_folder, character_folder, image_file), alphabet_folder, character_folder])\n",
    "\n",
    "# Create pandas dataframes for the train and test data\n",
    "train_df = pd.DataFrame(train_data, columns=['filepath', 'language', 'character'])\n",
    "test_df = pd.DataFrame(test_data, columns=['filepath', 'language', 'character'])\n",
    "\n",
    "# Combine the language and character columns to form the label column\n",
    "train_df['label'] = train_df['language'] + '_' + train_df['character']\n",
    "test_df['label'] = test_df['language'] + '_' + test_df['character']\n",
    "\n",
    "# Print the train and test dataframes\n",
    "print('Train dataframe:')\n",
    "print(train_df.head())\n",
    "print('\\nTest dataframe:')\n",
    "print(test_df.head())\n",
    "\n",
    "# create a csv file for train and test data\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecd4cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068e7906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/3mys0gn13yjg4kzxjyxb2gcm0000gp/T/ipykernel_93713/1328291207.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_data = np.array(train_data)\n",
      "/var/folders/s9/3mys0gn13yjg4kzxjyxb2gcm0000gp/T/ipykernel_93713/1328291207.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_data = np.array(test_data)\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.DataFrame(train_data, columns=['filepath', 'label'])\n",
    "# test_df = pd.DataFrame(test_data, columns=['filepath', 'label'])\n",
    "\n",
    "# Preprocessing the data\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "    img = cv2.imread(train_df['filepath'][i])\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    train_data.append([img, train_df['label'][i]])\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    img = cv2.imread(test_df['filepath'][i])\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    test_data.append([img, test_df['label'][i]])\n",
    "# print(train_data[0:10])\n",
    "# Converting the data to numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# Splitting the data into X_train, y_train, X_test, y_test\n",
    "X_train = np.array([i[0] for i in train_data])\n",
    "y_train = np.array([i[1] for i in train_data])\n",
    "X_test = np.array([i[0] for i in test_data])\n",
    "y_test = np.array([i[1] for i in test_data])\n",
    "\n",
    "# Normalizing the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b902c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 17:11:28.902314: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814/814 [==============================] - 34s 41ms/step - loss: 7.3638 - accuracy: 0.0012 - val_loss: 7.0673 - val_accuracy: 0.0064\n",
      "Epoch 2/10\n",
      "814/814 [==============================] - 37s 46ms/step - loss: 5.4913 - accuracy: 0.0698 - val_loss: 4.4127 - val_accuracy: 0.1405\n",
      "Epoch 3/10\n",
      "814/814 [==============================] - 37s 46ms/step - loss: 3.2166 - accuracy: 0.2873 - val_loss: 3.2153 - val_accuracy: 0.3039\n",
      "Epoch 4/10\n",
      "814/814 [==============================] - 39s 48ms/step - loss: 2.0772 - accuracy: 0.4816 - val_loss: 2.6179 - val_accuracy: 0.4103\n",
      "Epoch 5/10\n",
      "814/814 [==============================] - 38s 47ms/step - loss: 1.4394 - accuracy: 0.6083 - val_loss: 2.2912 - val_accuracy: 0.4793\n",
      "Epoch 6/10\n",
      "814/814 [==============================] - 39s 48ms/step - loss: 1.0439 - accuracy: 0.7080 - val_loss: 2.1249 - val_accuracy: 0.5211\n",
      "Epoch 7/10\n",
      "814/814 [==============================] - 44s 54ms/step - loss: 0.7620 - accuracy: 0.7771 - val_loss: 2.1304 - val_accuracy: 0.5311\n",
      "Epoch 8/10\n",
      "814/814 [==============================] - 44s 54ms/step - loss: 0.5803 - accuracy: 0.8219 - val_loss: 2.2709 - val_accuracy: 0.5491\n",
      "Epoch 9/10\n",
      "814/814 [==============================] - 40s 49ms/step - loss: 0.4454 - accuracy: 0.8596 - val_loss: 2.3947 - val_accuracy: 0.5535\n",
      "Epoch 10/10\n",
      "814/814 [==============================] - 42s 51ms/step - loss: 0.3503 - accuracy: 0.8847 - val_loss: 2.4903 - val_accuracy: 0.5601\n",
      "Train accuracy: 0.9250364899635315\n",
      "Test accuracy: 0.5601491928100586\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert categorical labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['label'])\n",
    "y_test = label_encoder.transform(test_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on train and test data\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea02c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "814/814 [==============================] - 41s 49ms/step - loss: 7.2123 - accuracy: 0.0027 - val_loss: 6.4635 - val_accuracy: 0.0126\n",
      "Epoch 2/10\n",
      "814/814 [==============================] - 40s 50ms/step - loss: 4.8802 - accuracy: 0.1064 - val_loss: 3.9577 - val_accuracy: 0.1864\n",
      "Epoch 3/10\n",
      "814/814 [==============================] - 41s 50ms/step - loss: 2.8156 - accuracy: 0.3431 - val_loss: 2.8407 - val_accuracy: 0.3444\n",
      "Epoch 4/10\n",
      "814/814 [==============================] - 41s 50ms/step - loss: 1.8673 - accuracy: 0.5196 - val_loss: 2.3655 - val_accuracy: 0.4399\n",
      "Epoch 5/10\n",
      "814/814 [==============================] - 39s 48ms/step - loss: 1.3511 - accuracy: 0.6295 - val_loss: 2.1613 - val_accuracy: 0.4869\n",
      "Epoch 6/10\n",
      "814/814 [==============================] - 40s 49ms/step - loss: 1.0183 - accuracy: 0.7076 - val_loss: 2.0351 - val_accuracy: 0.5219\n",
      "Epoch 7/10\n",
      "814/814 [==============================] - 42s 51ms/step - loss: 0.7659 - accuracy: 0.7717 - val_loss: 2.0709 - val_accuracy: 0.5445\n",
      "Epoch 8/10\n",
      "814/814 [==============================] - 40s 49ms/step - loss: 0.5874 - accuracy: 0.8175 - val_loss: 2.0734 - val_accuracy: 0.5594\n",
      "Epoch 9/10\n",
      "814/814 [==============================] - 42s 52ms/step - loss: 0.4619 - accuracy: 0.8553 - val_loss: 2.2200 - val_accuracy: 0.5553\n",
      "Epoch 10/10\n",
      "814/814 [==============================] - 40s 49ms/step - loss: 0.3646 - accuracy: 0.8817 - val_loss: 2.3943 - val_accuracy: 0.5640\n",
      "Train accuracy: 0.9186967015266418\n",
      "Test accuracy: 0.5640348196029663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert categorical labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['label'])\n",
    "y_test = label_encoder.transform(test_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on train and test data\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('Train accuracy:', train_acc)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e2627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f965ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a695721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db4ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c93064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c376de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfb822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
